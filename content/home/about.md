+++
# About/Biography widget.
widget = "about"
active = true
date = 2016-04-20T00:00:00

# Order that this section will appear in.
weight = 5

# List your academic interests.
# [interests]
#   interests = [
#     "Artificial Intelligence",
#     "Computational Linguistics",
#     "Information Retrieval"
#   ]

# List your qualifications (such as academic degrees).
# [[education.courses]]
#   course = "PhD in Artificial Intelligence"
#   institution = "Stanford University"
#   year = 2012

# [[education.courses]]
#   course = "MEng in Artificial Intelligence"
#   institution = "Massachusetts Institute of Technology"
#   year = 2009

# [[education.courses]]
#   course = "BSc in Artificial Intelligence"
#   institution = "Massachusetts Institute of Technology"
#   year = 2008
 
+++

I am a [Harvard Data Science](https://datascience.harvard.edu/) and [CRCS](https://crcs.seas.harvard.edu/) Postdoctoral Fellow at Harvard & MIT. My interests span the study of intelligence (natural), intelligence (artificial), and intelligence (moral). My goal is to understand how the human mind works with enough precision that it can be implemented on a computer. I also draw on insights from how people learn and think to engineer smarter and more human-like algorithms for artificial intelligence. 

I primarily research social intelligence where the scale, scope, and sophistication of humans is distinct in the natural world and where even a kindergartener exceeds the social competence of our best AI systems. I aim to reverse-engineer:

<!-- How do we infer what other people want and feel and how do we learn what they know?  -->
<!-- How do we learn about people such as who is friend or foe and who is good or bad? How do we learn from people what they and learn what they know?  -->
* **Social Learning:** How do we infer the hidden contents of other people's minds and learn what they know, want, and think? Or distinguish friend and foe based on just a few social interactions? How do we "download" knowledge from a cumulative culture or "upload" our discoveries through teaching and communication? Can we build machines with "Theory-of-Mind" that learn from people as quickly and intuitively as we do? 
* **Cooperation:** When do we cooperate with others and when do we compete? How do we coordinate actions and intentions to achieve together what none of us could achieve on our own? Can we build machines that collaborate with us as flexibly and fairly as a friend or colleague?
* **Morality:** What are moral values, how do we learn them so quickly, and why do we generalize them so widely? Where do our moral values come from and where are they going? How should we build moral machines? Will they amplify our biases or help us overcome them?

<!-- Should we program them to be moral and fair or have them learn our values like a child?  -->
Recent breakthroughs in cognitive science, made possible by new tools from machine learning and artificial intelligence, are allowing us to give formal answers to these questions for the first time. 
I build computational cognitive models of social intelligence using tools from Bayesian inference, reinforcement learning, and evolutionary game theory. These models give precise accounts of human social cognition and make fine-grained predictions that I test empirically in multi-agent behavioral experiments. At Harvard, I work with [Fiery Cushman](http://cushmanlab.fas.harvard.edu/), [Sam Gershman](http://gershmanlab.webfactional.com/), [Joe Henrich](https://henrich.fas.harvard.edu/), and [David Parkes](https://www.eecs.harvard.edu/~parkes/). At MIT I work with [David Rand](http://davidrand-cooperation.com/), [Rebecca Saxe](http://saxelab.mit.edu/) and [Josh Tenenbaum](http://web.mit.edu/cocosci/josh.html).


<!-- This mathematically formal approach achieves a dual goal: we can test fine-grained psychological theories of human social cognition with unprecedented fineness and close the gap between our best artificial intelligence and a socially savvy kindergartener.  -->
<!-- I use these tools to focus on the "why" and the "how" of our social intelligence: -->

I earned my PhD in Computational Cognitive Science from MIT, advised by [Josh Tenenbaum](http://web.mit.edu/cocosci/josh.html), where I received fellowships from the Hertz Foundation and NSF. Previously, I was a Marshall Scholar in Statistics at Oxford advised by [Tim Behrens](https://users.fmrib.ox.ac.uk/~behrens/), a Fulbright Fellow in Beijing with [Scott Rozelle](https://reap.fsi.stanford.edu/people/scott_rozelle), and before that was an undergraduate at Stanford where I worked with [John Huguenard](https://huguenard-lab.stanford.edu/wp1/). I am also Chief Scientist of [Diffeo](https://www.diffeo.com), a start-up that I co-founded. We build algorithms to help people find and fill gaps in their knowledge. I grew up in Santa Monica and outside of science, I enjoy surfing, skiing, and sushi. 

If you would like to get involved in research on these topics, please email me with your interests and background (as specific as possible) with resume/CV: <maxkleimanweiner@fas.harvard.edu> or <maxkw@mit.edu>. 

<!-- What are the _evolutionary_ origins (biological or cultural) of our moral and social knowledge and how do these evolved structures enable distinctively human cooperation? How is this knowledge rapidly learned with high fidelity during _development_, accumulating over generations? Finally, how is social and moral knowledge created and flexibly generalized and deployed _in the moment_, across an infinitude of possible situations and people?  -->
<!-- I use this framework to probe cognitive questions across three time-scales: evolutionary, developmental, and in the moment. First, I investigate the evolutionary origins of the cognitive structures that enable cooperation and support social learning. I then describe how these structures are used to learn social and moral knowledge rapidly during development, leading to the accumulation of knowledge over generations. Finally I show how this knowledge is used and generalized in the moment, across an infinitude of possible situations. -->
<!-- This framework is applied to a variety of cognitively challenging social inferences: determining the intentions of others, distinguishing who is friend or foe, and inferring the reputation of others all from just a single observation of behavior. It also answers how these inferences enable fair and reciprocal cooperation, the computation of moral permissibility, and moral learning. This framework predicts and explains human judgment and behavior measured in large-scale multi-person experiments.  -->

